{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"COMET_API_KEY\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Wikipedia page to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/MrBeast'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the text on the page\n",
    "text = soup.get_text()\n",
    "\n",
    "# Open a new file called 'output.txt' in write mode and store the file object in a variable\n",
    "with open('output.txt', 'w', encoding='utf-8') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(text)\n",
    "\n",
    "# load the document\n",
    "with open('./output.txt', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# define the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])\n",
    "\n",
    "# define the embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# use the text chunks and the embeddings model to fill our vector store\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking an agent with Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'd:\\\\Videolar\\\\Tirendaz\\\\English\\\\LangChain\\\\Projects\\\\agent-with-tools' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/tirendaz-academy/tracking-langchain-experiments/ab9ed69730ef445980fea5a5c88e474e\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m The comet_ml callback is currently in beta and is subject to change based on updates to `langchain`. Please report any issues to https://github.com/comet-ml/issue-tracking/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m MrBeast's age is likely mentioned somewhere in the context documents.\n",
      "Action: Search\n",
      "Action Input: \"MrBeast age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m25 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the calculator to find the answer to the second part of the question.\n",
      "Action: Calculator\n",
      "Action Input: 25 ^ 0.54\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 5.687057308780144\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: MrBeast is an American YouTuber and philanthropist born in May 7, 1998 and his age raised to the power of 0.54 is 5.687057308780144.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/tirendaz-academy/tracking-langchain-experiments/ab9ed69730ef445980fea5a5c88e474e\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_25% [4]   : (5.5, 61.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_50% [4]   : (5.5, 61.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_75% [4]   : (5.5, 61.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_count     : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_max [4]   : (5.5, 61.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_mean [4]  : (5.5, 61.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_min [4]   : (5.5, 61.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     automated_readability_index_std       : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_25% [4]            : (6.59, 30.52)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_50% [4]            : (6.59, 30.52)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_75% [4]            : (6.59, 30.52)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_count              : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_max [4]            : (6.59, 30.52)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_mean [4]           : (6.59, 30.52)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_min [4]            : (6.59, 30.52)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     coleman_liau_index_std                : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_25% [4]                      : (1.9, 2.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_50% [4]                      : (1.9, 2.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_75% [4]                      : (1.9, 2.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_count                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_max [4]                      : (1.9, 2.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_mean [4]                     : (1.9, 2.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_min [4]                      : (1.9, 2.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crawford_std                          : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_25% [4]  : (8.49, 35.37)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_50% [4]  : (8.49, 35.37)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_75% [4]  : (8.49, 35.37)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_count    : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_max [4]  : (8.49, 35.37)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_mean [4] : (8.49, 35.37)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_min [4]  : (8.49, 35.37)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dale_chall_readability_score_std      : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_25% [4]               : (2.0, 7.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_50% [4]               : (2.0, 7.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_75% [4]               : (2.0, 7.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_count                 : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_max [4]               : (2.0, 7.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_mean [4]              : (2.0, 7.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_min [4]               : (2.0, 7.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     difficult_words_std                   : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_25% [4]              : (41.78, 107.03)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_50% [4]              : (41.78, 107.03)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_75% [4]              : (41.78, 107.03)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_count                : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_max [4]              : (41.78, 107.03)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_mean [4]             : (41.78, 107.03)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_min [4]              : (41.78, 107.03)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fernandez_huerta_std                  : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_25% [4]          : (6.4, 17.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_50% [4]          : (6.4, 17.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_75% [4]          : (6.4, 17.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_count            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_max [4]          : (6.4, 17.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_mean [4]         : (6.4, 17.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_min [4]          : (6.4, 17.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_kincaid_grade_std              : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_25% [4]           : (-24.64, 72.66)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_50% [4]           : (-24.64, 72.66)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_75% [4]           : (-24.64, 72.66)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_count             : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_max [4]           : (-24.64, 72.66)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_mean [4]          : (-24.64, 72.66)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_min [4]           : (-24.64, 72.66)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flesch_reading_ease_std               : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_25% [4]                : (15.7, 70.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_50% [4]                : (15.7, 70.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_75% [4]                : (15.7, 70.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_count                  : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_max [4]                : (15.7, 70.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_mean [4]               : (15.7, 70.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_min [4]                : (15.7, 70.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gulpease_index_std                    : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_25% [4]                   : (5.7, 27.87)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_50% [4]                   : (5.7, 27.87)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_75% [4]                   : (5.7, 27.87)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_count                     : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_max [4]                   : (5.7, 27.87)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_mean [4]                  : (5.7, 27.87)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_min [4]                   : (5.7, 27.87)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gunning_fog_std                       : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_25% [4]              : (0.38, 49.46)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_50% [4]              : (0.38, 49.46)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_75% [4]              : (0.38, 49.46)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_count                : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_max [4]              : (0.38, 49.46)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_mean [4]             : (0.38, 49.46)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_min [4]              : (0.38, 49.46)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gutierrez_polini_std                  : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_25% [4]         : (2.0, 7.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_50% [4]         : (2.0, 7.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_75% [4]         : (2.0, 7.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_count           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_max [4]         : (2.0, 7.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_mean [4]        : (2.0, 7.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_min [4]         : (2.0, 7.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     linsear_write_formula_std             : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     openai_completion_tokens [4]          : (26, 132)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     openai_prompt_tokens [4]              : (701, 2454)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     openai_total_tokens [4]               : (727, 2586)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_25% [4]                         : (-108.56, 76.41)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_50% [4]                         : (-108.56, 76.41)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_75% [4]                         : (-108.56, 76.41)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_count                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_max [4]                         : (-108.56, 76.41)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_mean [4]                        : (-108.56, 76.41)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_min [4]                         : (-108.56, 76.41)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     osman_std                             : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_25%                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_50%                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_75%                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_count                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_max                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_mean                       : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_min                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     smog_index_std                        : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_25% [4]               : (37.7, 106.93)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_50% [4]               : (37.7, 106.93)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_75% [4]               : (37.7, 106.93)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_count                 : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_max [4]               : (37.7, 106.93)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_mean [4]              : (37.7, 106.93)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_min [4]               : (37.7, 106.93)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     szigriszt_pazos_std                   : nan\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     openai_api_base     : https://api.openai.com/v1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     openai_api_type     : open_ai\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     openai_organization : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model__type             : openai\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_frequency_penalty : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_logit_bias        : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_max_tokens        : 256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_model_name        : text-davinci-003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_n                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_presence_penalty  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_request_timeout   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_temperature       : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_top_p             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 5 (29.13 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataframe                    : 1 (11.86 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element                : 1 (1.91 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     text-sample                  : 21\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.callbacks import CometCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "comet_callback = CometCallbackHandler(\n",
    "    complexity_metrics=True,\n",
    "    project_name=\"tracking-langchain-experiments\",\n",
    "    stream_logs=True,\n",
    "    tags=[\"qa\"],\n",
    ")\n",
    "\n",
    "callbacks = [StdOutCallbackHandler(), comet_callback]\n",
    "\n",
    "users_question = \"Who is MrBeast? What is his age raised to the power of 0.54?\"\n",
    "\n",
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=users_question,\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "\n",
    "You are a chatbot who loves helping people! Given the context sections below, \n",
    "answer the question using only the context provided. If you're not sure \n",
    "and the answer not explicitly writting in the documentation,  \n",
    "just say \"Sorry, I don't know how to assist with this.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "# ask the defined LLM\n",
    "llm = OpenAI(temperature=1, callbacks=callbacks)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callbacks=callbacks)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    callbacks=callbacks,\n",
    "    verbose=True,\n",
    ")\n",
    "agent.run(prompt_text)\n",
    "comet_callback.flush_tracker(agent, finish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "- [All You Need to Know to Build Your First LLM App](https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)\n",
    "- [Conversation QA Gradio ](https://github.com/hwchase17/conversation-qa-gradio/tree/master)\n",
    "\n",
    "Thanks for reading. Let's connect [YouTube](http://youtube.com/tirendazacademy) | [Medium](http://tirendazacademy.medium.com) | [Twitter](http://twitter.com/tirendazacademy) | [Linkedin](https://www.linkedin.com/in/tirendaz-academy) ðŸ˜Ž"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
